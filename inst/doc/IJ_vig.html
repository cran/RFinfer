<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Cole Brokamp" />

<meta name="date" content="2016-06-10" />

<title>Infinitesimal Jackknife</title>






<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Infinitesimal Jackknife</h1>
<h4 class="author"><em>Cole Brokamp</em></h4>
<h4 class="date"><em>2016-06-10</em></h4>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: { 
            autoNumber: "AMS",
            formatNumber: function (n) {return ''+n}
      } 
  }
});
</script>
<p>Bootstrap sampling, subsampling, and the jackknife all rely on estimating the variance of a statistic by using the variability between resamples rather than using statistical distributions. This vignette will cover the jackknife and how it is applied to the resampling distribution to generate variance estimates for random forest predictions.</p>
<div id="ordinary-jackknife" class="section level2">
<h2>Ordinary Jackknife</h2>
<p>The ordinary jackknife is a resampling method useful for estimating the variance or bias of a statistic. The jackknife estimate of a statistic can be found be by repeatedly calculating the statistic, each time leaving one observation from the sample out and averaging all estimates. The variance of the estimate can be found by calculating the variance of the jackknifed estimates:</p>
\begin{equation}
\hat{V}_{J} = \frac{n-1}{n}\sum\limits_{i=1}^{n} \left(\hat{\theta}_{(-i)} - \hat{\theta}_{(\cdot)}\right)^2
\end{equation}
<p>where <span class="math inline">\(n\)</span> is the total sample size, <span class="math inline">\(\hat{\theta}_{(-i)}\)</span> is the statistic estimated without using the <span class="math inline">\(i^{th}\)</span> observation, and <span class="math inline">\(\hat{\theta}_{(\cdot)}\)</span> is the average of all jackknife estimates.</p>
</div>
<div id="jackknife-after-bootstrap" class="section level2">
<h2>Jackknife After Bootstrap</h2>
<p>The ordinary jackknife is extended for use with bagging by applying it to the bootstrap distribution (Efron2014). Instead of leaving out one observation at a time, the existing bootstrap samples are used and the statistic is calculated based on all resamples which do not use the <span class="math inline">\(i^{th}\)</span> observation:</p>
\begin{equation}
\hat{V}_{JB} = \frac{n-1}{n}\sum\limits_{i=1}^{n} \left(\bar{t^*}_{(-i)}(x) - \bar{t^*}_{(\cdot)}(x)\right)^2
\end{equation}
<p>where <span class="math inline">\(\bar{t^*}_{(-i)}(x)\)</span> is the average of <span class="math inline">\(t^*(x)\)</span> over all bootstrap samples not containing the <span class="math inline">\(i^{th}\)</span> example and <span class="math inline">\(\bar{t^*}_{(\cdot)}(x)\)</span> is the mean of all <span class="math inline">\(\bar{t^*}_{(i)}(x)\)</span>.</p>
</div>
<div id="infinitesimal-jackknife-and-resampling" class="section level2">
<h2>Infinitesimal Jackknife and Resampling</h2>
<p>As opposed to <span class="math inline">\(\hat{V}_{J}\)</span> and <span class="math inline">\(\hat{V}_{JB}\)</span>, where the behavior of a statistic is studied after removing one or more observations at a time, the infinitesimal jackknife (IJ) looks at the behavior of a statistic after down-weighting each observation by an infinitesimal amount (Jaeckel1972). Adapted from Efron (2014), the following is a gentle introduction to the idea.</p>
<p>Define</p>
\begin{equation}
t_i^*=t(\mathbf{y}_i^*)~~~~~~~~~~~~\mathbf{y}_i^*=(y_{i1}^*,y_{i2}^*,...,y_{ik}^*,...,y_{in}^*)
\end{equation}
<p>as the <span class="math inline">\(i^{th}\)</span> calculation of a statistic based on the <span class="math inline">\(i^{th}\)</span> bootstrap sample, and</p>
\begin{equation}
Y^*_{ij}=\#\{y^*_{ik}=y_j\}
\end{equation}
<p>as the number of times that the original data point <span class="math inline">\(y_j\)</span> appears in the <span class="math inline">\(i^{th}\)</span> bootstrap sample <span class="math inline">\(\mathbf{y}^*_i\)</span>. Then, the count vector <span class="math inline">\(\mathbf{Y}_i^*=(Y_{i1}^*,Y_{i2}^*,...,Y_{ik}^*,...,Y_{in}^*)\)</span> forms a multinomial distribution with <span class="math inline">\(n\)</span> draws on <span class="math inline">\(n\)</span> categories, each having a probability of <span class="math inline">\(1/n\)</span>. Using the mean and variance of the multinomial distribution, we can say that</p>
\begin{equation}
\mathbf{Y}^*_i \sim (\mathbf{1}_n,\mathbf{I}-\mathbf{1}_n\mathbf{1}_n^T/n).
\end{equation}
<p>By fixing the original data and writing the bootstrap replication statistic as function of the count vector, we can define the ideal smoothed bootstrap estimate <span class="math inline">\(S_0\)</span> as the multinomial expectation of <span class="math inline">\(T(\mathbf{Y}^*)\)</span>.</p>
\begin{equation}\label{eq:S_0}
S_0=E[T(\mathbf{Y}^*)],~~~\mathbf{Y}^* \sim \text{Mult}_n(n,\mathbf{p}_0),
\end{equation}
<p>with <span class="math inline">\(\mathbf{p}_0=(1/n,1/n,...,1/n)\)</span>. Extending this to a probability vector of <span class="math inline">\(\mathbf{p}=(p_1,p_2,...,p_n)\)</span> leads to</p>
\begin{equation}\label{eq:S(p)}
S(\mathbf{p})=E[T(\mathbf{Y}^*)],~~~\mathbf{Y}^* \sim \text{Mult}_n(n,\mathbf{p}),
\end{equation}
<p>Using the delta method, we can define the directional derivative as</p>
\begin{equation}\label{eq:S_j}
\dot{S}_j=\lim\limits_{\epsilon\rightarrow0}\frac{S(\mathbf{p}_0+\epsilon(\pmb{\delta}_j - \mathbf{p}_0))-S(\mathbf{p}_0)}{\epsilon}
\end{equation}
<p>where <span class="math inline">\(\pmb{\delta}_j\)</span> is the <span class="math inline">\(j^{th}\)</span> coordinate vector with all zeros except for a one in the <span class="math inline">\(j^{th}\)</span> place. We can then use the delta method to estimate the standard deviation of <span class="math inline">\(s_0\)</span></p>
\begin{equation}
\frac{\left(\sum_{j=1}^{n}\dot{S}_j^2\right)^{1/2}}{n}
\end{equation}
<p>In order to reduce this back into terms of <span class="math inline">\(\mathbf{Y}^*\)</span>, we define <span class="math inline">\(w_i(\mathbf{p})\)</span> as the probabilities of <span class="math inline">\(\mathbf{Y}^*\)</span> in Equation 7 divided by the probabilities of <span class="math inline">\(\mathbf{Y}^*\)</span> in Equation 6,</p>
\begin{equation}
w_i(\mathbf{p})=\prod_{k=1}^{n}(np_k)^{Y^*_{ik}},
\end{equation}
<p>such that</p>
\begin{equation}\label{eq:weights}
S(\mathbf{p})=\sum_{i=1}^{B}w_i(\mathbf{p})t_i^*/B
\end{equation}
<p>For <span class="math inline">\(\mathbf{p}(\epsilon)=\mathbf{p}_0+\epsilon(\mathbf{\delta}_j-\mathbf{p}_0)\)</span> as in Equation 8,</p>
\begin{equation}
w_i(\mathbf{p})=(1+(n-1)\epsilon)^{Y^*_{ij}}(1-\epsilon)^{\sum_{k\neq j}Y^*_{ik}}
\end{equation}
<p>Letting <span class="math inline">\(\epsilon \rightarrow 0\)</span> results in</p>
\begin{equation}
w_i(\mathbf{p})\doteq 1 + n\epsilon (Y^*_{ij}-1)
\end{equation}
<p>Substituting this back into Equation 11 gives</p>
\begin{equation}
S(\mathbf{p}(\epsilon)) \doteq \sum_{i=1}^{B}[1+n\epsilon(Y^*_{ij}-1)]t^*_i/B = s_0 + n \epsilon~\text{cov}_j
\end{equation}
<p>Using Equation 8 defines <span class="math inline">\(\dot{S}_j=n~\text{cov}_j\)</span>. Thus, the IJ estimated variance of a bagged predictor is</p>
\begin{equation}
\hat{V}_{IJ}=\sum_{j=1}^{n}\text{cov}_j
\end{equation}
<p>or the covariance between the predictions and the number of times each sample was used in the resamples.</p>
</div>
<div id="random-forest-prediction-variance" class="section level2">
<h2>Random Forest Prediction Variance</h2>
<p>Wager et al. (2014a) have recently extended this idea by applying the IJ to random forest predictions. Based on using subsamples rather than bootstrap samples, they have shown that the variance of random forest predictions can be consistently estimated. Here the IJ variance estimator is applied to the resampling distribution for a new prediction point:</p>
\begin{equation}
\label{eq:IJ}
\hat{V}_{IJ} = \sum\limits_{i=1}^{n} Cov_*\left[T(x; Z^*_1,...,Z^*_n),N^*_i\right]
\end{equation}
<p>where <span class="math inline">\(T(x; Z^*_1,...,Z^*_n)\)</span> is the prediction of the tree <span class="math inline">\(T\)</span> for the test point <span class="math inline">\(x\)</span> based on the subsample <span class="math inline">\(Z^*_1,...,Z^*_n\)</span> and <span class="math inline">\(N^*_i\)</span> is the number of times <span class="math inline">\(Z_i\)</span> appears in the subsample. Furthermore, random forest predictions are asymptotically normal given that the underlying trees are based on subsampling and that the subsample size <span class="math inline">\(s\)</span> scales as <span class="math inline">\(s(n)/n=o(log(n)^{-p})\)</span>, where <span class="math inline">\(n\)</span> the is number of training examples and <span class="math inline">\(p\)</span> is the number of features (Wager2014b).</p>
<p>Because <span class="math inline">\(\hat{V}_{IJ}\)</span> is calculated in practice with a finite number of trees <span class="math inline">\(B\)</span>, it is inherently associated with Monte Carlo error. Although this error can be decreased by using a large <span class="math inline">\(B\)</span>, a correction has been suggested (Wager2014a):</p>
\begin{equation}
\label{eq:IJB}
\hat{V}_{IJ}^B = \sum\limits_{i=1}^{n}C_i^2 - \frac{s(n-s)}{n}\frac{\hat{v}}{B}
\end{equation}
<p>where <span class="math inline">\(C_i = \frac{1}{B}\sum\limits_{b=1}^{B}(N^*_{bi} - s/n)(T^*_b - \bar{T}^*)\)</span> and <span class="math inline">\(\hat{v} = \frac{1}{B}\sum\limits_{b=1}^{B}(T^*_b - \bar{T}^*)^2\)</span>.</p>
<p>This is essentially a Monte Carlo estimate of Equation  with a bias correction subtracted off. These estimates are asymptotically normal given a few key conditions, one of which is that the underlying trees are honest (Wager2014b). Simulation experiments using sub bagged random forests have shown that these variance estimates are biased (Wager 2014b) and this is likely due to the fact that the underlying trees are not honest. Recently, we have shown that using conditional inference trees (an honest tree type) increases the accuracy of IJ estimator for random forest prediction variance (Brokamp2016).</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Efron B. Estimation and accuracy after model selection. Journal of the American Statistical Association. 2014 Jul 3;109(507):991-1007.</p>
<p>Jaeckel LA. The infinitesimal jackknife. 1972.</p>
<p>Wager S, Hastie T, Efron B. Confidence intervals for random forests: The jackknife and the infinitesimal jackknife. The Journal of Machine Learning Research. 2014 Jan 1;15(1):1625-51.</p>
<p>Wager S. Asymptotic theory for random forests. arXiv preprint arXiv:1405.0352. 2014 May 2.</p>
<p>Brokamp C, Rao MB, Ryan P, Jandarov J. A comparison of resampling and recursive partitioning methods in random forest for estimating the asymptotic variance using the infinitesimal jackknife. Submitted.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
